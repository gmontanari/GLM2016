---
title: "GLM: Proceso de Inferencia - Distribuciones iniciales informativas, no informativas y conjugadas"
output: html_notebook
---

Esto depende de si tenemos (Informativas) o no conocimiento (No informativas).
Las distribuciones conjugadas no tienen que ver con la cantidad de información, sino con la simplicidad para obtener la distribución final a partir de la distribución inicial.

Volviendo a Informativas o No Informativas, de acuerdo al proceso que veniamos viendo anteriormente, del proceso de Inferencia Normal- Normal:

Tenemos lo siguiente:
  $x1,...,xn$ es una m.a.
  
  $\bar{X} = (1/n)\Sigma xi$
  
  $E(\bar{X})= \mu$
  
  $Var(\bar{X})= (\varrho^2/n)$
  

```{r}
# Datos
xbar<-40.9533
sig2<-4
n<-3
# Distribución inicial del parámetro theta
th0<-39
sig20<-219.47
# Area de graficación
y<-seq(35,45,length.out=200)
# Armo la normal de theta, centrada en theta0=39 y varianza=219.47
f0y<-dnorm(y,th0,sqrt(sig20))
# Armo la normal de los datos - verosimilitud -, en este caso son 3, centrada en xbar=40.9533 y varianza=4
liky<-dnorm(y,xbar,sqrt(sig2/n))
# Armo la distribución final conjugada tomando la base de la inicial con los datos
sig21<-1/(n/sig2+1/sig20)
th1<-sig21*(n/sig2*xbar+th0/sig20)
f1y<-dnorm(y,th1,sqrt(sig21))

# Grafico las 3 juntas
ymax<-max(f0y,liky,f1y)
plot(y,f0y,ylim=c(0,ymax),type="l")
lines(y,liky,lty=2,col=2)
lines(y,f1y,lty=3,col=3)
```

### Esta gráfica tiene en principio tres densidades:
Una de color negro: la densidad inicial, que está alrededor de 39, con una varianza de 219.47 (Una varianza muy grande indica que sé poco). Con esta escala se ve en este intervalo como una uniforme, es decir, no informativa.

Una roja (verosimilitud - de los datos generados por mi proceso)

Una verde ( distribución final) encimadas, porque la final en este caso depende por completos de la verosimilitud: LOS DATOS. 

Entonces, en principio, mi distribuciòn inicial, que era informativa, porque tenia una media y una varianza, se vuelve no informativa respecto de mis datos, porque no está influyendo en el proceso de inferencia. 

Aunque tenga poquitos datos le voy a creer mucho a mis datos.

Las distribuciones inciales no informativas, son aquellas que no dan información relevante al problema y no influyen en el proceso de inferencia.

###Proceso de aprendizaje bernoulli-beta-

Vamos a simular datos que no tenemos: 10 datos Ber con parametro 0.6, los guardamos en la var x y hacemos un histograma.

```{r}
###Simulacion de datos Bernoulli
theta0 <- 0.6
n <- 10
x<-rbinom(n,1,theta0)
hist(x,freq=FALSE)
```

En mi histograma solo tengo 0 y 1, resultados de la Bernoulli.

Ahora tengo mi distribución inicial Beta con parametros a=1, b=1

```{r}
#Distribucion inicial para theta
a <- 1
b <- 1
theta<-seq(0,1,,100)
plot(theta,dbeta(theta,a,b),type="l")
```

Mi distribucion inicial es una uniforme.

Como es conjugada la distribución final es otra Beta con parametros a1,b1

```{r}
#Distribucion final
a1 <- a + sum(x)
b1 <- b + n - sum(x)
plot(theta,dbeta(theta,a1,b1),type="l")
```

Me dió asimetrica porque tengo distintos valores de 0 (4) y 1 (6)

Ahora graficamos la incial y la final

```{r}
#Ambas
theta<-seq(0,1,,100)
ymax <- max(dbeta(theta,a,b),dbeta(theta,a1,b1))
plot(theta,dbeta(theta,a,b),type="l",ylim=c(0,ymax))
lines(theta,dbeta(theta,a1,b1),col=2)
abline(v=theta0,col=4)
```

La Linea negra es la dist inicial

La linea roja es la dist final

La linea azul es el verdadero valor de Theta, que solo Dios conoce y nosotros estamos jugando a conocerlo.

Con 10 datos, mis dist final me dice que esta centrada en 0.6 y por ahi anda theta.

## Aproximacion normal asintotica: 

Estamos aproximando la distribución final, con una normal, a traves de las fórmulas de esta aproximacion, con la media y la varianza correspondientes.

```{r}
#Ambas
theta<-seq(0,1,,100)
ymax <- max(dbeta(theta,a,b),dbeta(theta,a1,b1))
plot(theta,dbeta(theta,a,b),type="l",ylim=c(0,ymax))
lines(theta,dbeta(theta,a1,b1),col=2)
abline(v=theta0,col=4)

#Aproximacion normal asintotica
mu <- (a1-1)/(a1+b1-2)
sig2 <- (a1-1)*(b1-1)/(a1+b1-2)^3
lines(theta,dnorm(theta,mu,sqrt(sig2)),col=3)
```

La funcion verde es la Distribución Normal Asistótica (verde). Se parece mucho a la Distribución final(roja), aunque no es igual.

## Aumentamos el tamaño de muestra, ya que 10 no es muy grande, para ver como mejora la aproximación

```{r}
###Simulacion de datos Bernoulli
theta0 <- 0.6
n <- 20
x<-rbinom(n,1,theta0)

#Distribucion inicial para theta
a <- 1
b <- 1
theta<-seq(0,1,,100)

#Distribucion final
a1 <- a + sum(x)
b1 <- b + n - sum(x)

#Ambas
theta<-seq(0,1,,100)
ymax <- max(dbeta(theta,a,b),dbeta(theta,a1,b1))
plot(theta,dbeta(theta,a,b),type="l",ylim=c(0,ymax))
lines(theta,dbeta(theta,a1,b1),col=2)
abline(v=theta0,col=4)
#Aproximacion normal asintotica
mu <- (a1-1)/(a1+b1-2)
sig2 <- (a1-1)*(b1-1)/(a1+b1-2)^3
lines(theta,dnorm(theta,mu,sqrt(sig2)),col=3)
```

Con 20, la normal(verde) que aproxima se va a parecer mas a la dist final(roja). No veo que este centrada.


# ¿Que pasa si aumento a 50?

```{r}
###Simulacion de datos Bernoulli
theta0 <- 0.6
n <- 50
x<-rbinom(n,1,theta0)

#Distribucion inicial para theta
a <- 1
b <- 1
theta<-seq(0,1,,100)

#Distribucion final
a1 <- a + sum(x)
b1 <- b + n - sum(x)

#Ambas
theta<-seq(0,1,,100)
ymax <- max(dbeta(theta,a,b),dbeta(theta,a1,b1))
plot(theta,dbeta(theta,a,b),type="l",ylim=c(0,ymax))
lines(theta,dbeta(theta,a1,b1),col=2)
abline(v=theta0,col=4)
#Aproximacion normal asintotica
mu <- (a1-1)/(a1+b1-2)
sig2 <- (a1-1)*(b1-1)/(a1+b1-2)^3
lines(theta,dnorm(theta,mu,sqrt(sig2)),col=3)
```

La aproximación normal asintótica(verde) es muy buena con respecto a la distribución final(roja).

# Con 100 datos....

```{r}
###Simulacion de datos Bernoulli
theta0 <- 0.6
n <- 100
x<-rbinom(n,1,theta0)

#Distribucion inicial para theta
a <- 1
b <- 1
theta<-seq(0,1,,100)

#Distribucion final
a1 <- a + sum(x)
b1 <- b + n - sum(x)

#Ambas
theta<-seq(0,1,,100)
ymax <- max(dbeta(theta,a,b),dbeta(theta,a1,b1))
plot(theta,dbeta(theta,a,b),type="l",ylim=c(0,ymax))
lines(theta,dbeta(theta,a1,b1),col=2)
abline(v=theta0,col=4)
#Aproximacion normal asintotica
mu <- (a1-1)/(a1+b1-2)
sig2 <- (a1-1)*(b1-1)/(a1+b1-2)^3
lines(theta,dnorm(theta,mu,sqrt(sig2)),col=3)
```

# Estamos ilustrando aqui dos cosas:
  . La aproximación normal asintótica
  . En el proceso de inferencia, cuando n tiende a infinito, llegamos al verdadero valor de theta.
  . Aun tenemos una cierta incertidumbre del valor de theta (la varianza)
  
# ¿Qué pasa con n = 500?

```{r}
###Simulacion de datos Bernoulli
theta0 <- 0.6
n <- 500
x<-rbinom(n,1,theta0)

#Distribucion inicial para theta
a <- 1
b <- 1
theta<-seq(0,1,,100)

#Distribucion final
a1 <- a + sum(x)
b1 <- b + n - sum(x)

#Ambas
theta<-seq(0,1,,100)
ymax <- max(dbeta(theta,a,b),dbeta(theta,a1,b1))
plot(theta,dbeta(theta,a,b),type="l",ylim=c(0,ymax))
lines(theta,dbeta(theta,a1,b1),col=2)
abline(v=theta0,col=4)
#Aproximacion normal asintotica
mu <- (a1-1)/(a1+b1-2)
sig2 <- (a1-1)*(b1-1)/(a1+b1-2)^3
lines(theta,dnorm(theta,mu,sqrt(sig2)),col=3)
```

La verde y la roja estan ya superpuestas, pero nos estamos cerrando más, es decir, la varianza esta disminuyendo, asi como la incertidumbre.

# ¿qué sucede con n=100000?

```{r}
###Simulacion de datos Bernoulli
theta0 <- 0.6
n <- 100000
x<-rbinom(n,1,theta0)

#Distribucion inicial para theta
a <- 1
b <- 1
theta<-seq(0,1,,100)

#Distribucion final
a1 <- a + sum(x)
b1 <- b + n - sum(x)

#Ambas
theta<-seq(0,1,,100)
ymax <- max(dbeta(theta,a,b),dbeta(theta,a1,b1))
plot(theta,dbeta(theta,a,b),type="l",ylim=c(0,ymax))
lines(theta,dbeta(theta,a1,b1),col=2)
abline(v=theta0,col=4)
#Aproximacion normal asintotica
mu <- (a1-1)/(a1+b1-2)
sig2 <- (a1-1)*(b1-1)/(a1+b1-2)^3
lines(theta,dnorm(theta,mu,sqrt(sig2)),col=3)
```

## Conclusión: a partir de 30 la aproximación normal asintótica ya es bastante buena para la distribución final.

# Aproximación numérica: Monte Carlo

La construyo a través de una función que tenga colas más pesadas que la quiero aproximar, para poder recuperar la mayor cantidad de valores posibles para la distribución final.
A través de las funciones S() aproximo el valor de la integral de la f() que desconozco.

```{r}
# --- Aproximaci?n Monte Carlo --- 
#-Ejemplo 1-
x<-seq(-2,4,,1000)
f<-function(x){
  out <- 5-(x-1)^2
  out <- ifelse (x < -1 | x>3,0,out)
  out
}
plot(x,f(x)*3/44,type="l",ylim=c(0,0.5))
lines(x,dnorm(x,0,1),lty=2,col=2)
lines(x,dnorm(x,1,2/3),lty=3,col=3)
lines(x,dnorm(x,1,1),lty=4,col=4)
lines(x,dnorm(x,1,3),lty=5,col=5)
```

Lo que yo quiero integrar es mi f() que es la linea negra.

$f(x) = (5-(x-1)^2)$

La quiero integrar de -1 a 3, porque solo ese intervalo es el que me intersa, como la distribución inicial. 

$\int_{-1}^{3}(3/44)(5-(x-1)^2) dx$

Es una parabola que abre hacia abajo, centrada en 1 y subida en 5 unidades.

Nos pidió hacer la integral a mano de:

$\int_{-1}^{3}(3/44)(5-(x-1)^2) dx$ = 44/3 = 14.66

Porque ese es el verdadero valor que yo quiero obtener.

### ¿Como la voy a integrar?

A traves de 3 propuestas:

-S(1) Rojo - Normal Standard: Asigna mucha masa al centro, pero me quedo corta en la cola. (respecto de los valores de la negra)

-S(2) Verde - Normal no Standard: Igual que la verde, centrada en 1, pero con una cola mas pesada y asi recupero mas valores de f()

-S(3) Azul: Otra opción

-S(4) (...): como S(3) pero con colas más pesadas


### Entonces voy a simular de esas S(.)'s y quiero ver:
  - Que tanto se parecen al valor real
  - Cuál es el error de Monte Carlo

###Caso 1: Normal Estandard (Rojo): donde entre (-3,3) casi tengo el 99% de mis observaciones

Empiezo con 100

```{r}
#Caso 1: S=Normal estandar (0,1)
mu<-0
sig<-1
N<-100
# Genero entonces 100000 muestras de una normal estandard - rnorm -
y<-rnorm(N,mu,sig)

# Calculo la integral a traves de la media muestral del cociente entre f() y S().
# S() en este caso es la función de densidad de la N(0,1) - dnorm - de donde simulé las thetas.
I1<-mean(f(y)/dnorm(y,mu,sig))

# Calculo mi error estandar, que es la desviaciòn estándar muestral de estas cosas
eI1<-sd(f(y)/dnorm(y,0,1))/sqrt(N)
eS1 <-(c(I1,eI1))
```

Veo el histograma del cociente de las funciones para ver como se distribuyeron los valores

```{r}
hist(f(y)/dnorm(y,0,1))
```

###Caso 2: Normal No Estandard (Verde): donde entre (-3,3) casi tengo el 99% de mis observaciones

```{r}
#Caso 2: S=Normal no estandar
mu<-1
sig<-2/3
N<-100
y<-rnorm(N,mu,sig)
I2<-mean(f(y)/dnorm(y,mu,sig))
eI2<-sd(f(y)/dnorm(y,mu,sig))/sqrt(N)
eS2 <-(c(I2,eI2))
```

Veo el histograma del cociente de las funciones para ver como se distribuyeron los valores

```{r}
hist(f(y)/dnorm(y,mu,sig))
```

###Caso 3: Normal No Estandard (Azul): donde entre (-3,3) casi tengo el 99% de mis observaciones

```{r}
#Caso 3: S=Normal no estandar
mu<-1
sig<-1
N<-100
y<-rnorm(N,mu,sig)
I3<-mean(f(y)/dnorm(y,mu,sig))
eI3<-sd(f(y)/dnorm(y,mu,sig))/sqrt(N)
eS3 <-(c(I1,eI1))
```

Veo el histograma del cociente de las funciones para ver como se distribuyeron los valores

```{r}
hist(f(y)/dnorm(y,mu,sig))
```

### Caso 4: Normal No Estandard (Azul): donde entre (-3,3) casi tengo el 99% de mis observaciones, pero tiene colas mas pesadas que S(3) porque la varianza cambia de 1 a 3.

```{r}
#Caso 4: S=Normal no estandar
mu<-1
sig<-3
N<-100
y<-rnorm(N,mu,sig)
I4<-mean(f(y)/dnorm(y,mu,sig))
eI4<-sd(f(y)/dnorm(y,mu,sig))/sqrt(N)
eS4 <-(c(I4,eI4))
```


Veo el histograma del cociente de las funciones para ver como se distribuyeron los valores

```{r}
hist(f(y)/dnorm(y,mu,sig))
```

### Conclusiones para N = 100

Veo los diferentes valores:
S(1), S(2), S(3), S(4)
```{r}
print(eS1)
print(eS2)
print(eS3)
print(eS4)
```

Estos tres estimadores son insesgados, lo que quiere decir que cuando "n"" tienda a infinito los tres van a llegar al verdadero valor 14.66.

Pero lo que mas vale es el error de estimación, el menor, en este caso el error de la tercera es menor, y por lo tanto es mejor aproximación.

## Simulación 2: simulo 10000 valores 


###Caso 1: Normal Estandard (Rojo): donde entre (-3,3) casi tengo el 99% de mis observaciones

```{r}
#Caso 1: S=Normal estandar (0,1)
mu<-0
sig<-1
N<-10000
# Genero entonces 100000 muestras de una normal estandard - rnorm -
y<-rnorm(N,mu,sig)

# Calculo la integral a traves de la media muestral del cociente entre f() y S().
# S() en este caso es la función de densidad de la N(0,1) - dnorm - de donde simulé las thetas.
I1<-mean(f(y)/dnorm(y,mu,sig))

# Calculo mi error estandar, que es la desviaciòn estándar muestral de estas cosas
eI1<-sd(f(y)/dnorm(y,0,1))/sqrt(N)
eS1 <-(c(I1,eI1))
```

Veo el histograma del cociente de las funciones para ver como se distribuyeron los valores

```{r}
hist(f(y)/dnorm(y,mu,sig))
```

###Caso 2: Normal No Estandard (Verde): donde entre (-3,3) casi tengo el 99% de mis observaciones

```{r}
#Caso 2: S=Normal no estandar
mu<-1
sig<-2/3
N<-10000
y<-rnorm(N,mu,sig)
I2<-mean(f(y)/dnorm(y,mu,sig))
eI2<-sd(f(y)/dnorm(y,mu,sig))/sqrt(N)
eS2 <-(c(I2,eI2))
```

Veo el histograma del cociente de las funciones para ver como se distribuyeron los valores

```{r}
hist(f(y)/dnorm(y,mu,sig))
```

###Caso 3: Normal No Estandard (Azul): donde entre (-3,3) casi tengo el 99% de mis observaciones

```{r}
#Caso 3: S=Normal no estandar
mu<-1
sig<-1
N<-10000
y<-rnorm(N,mu,sig)
I3<-mean(f(y)/dnorm(y,mu,sig))
eI3<-sd(f(y)/dnorm(y,mu,sig))/sqrt(N)
eS3 <-(c(I3,eI3))
```

Veo el histograma del cociente de las funciones para ver como se distribuyeron los valores

```{r}
hist(f(y)/dnorm(y,mu,sig))
```

###Caso 4: Normal No Estandard (Azul): donde entre (-3,3) casi tengo el 99% de mis observaciones, pero tiene colas mas pesadas que S(3) porque la varianza cambia de 1 a 2.

```{r}
#Caso 4: S=Normal no estandar
mu<-1
sig<-3
N<-10000
y<-rnorm(N,mu,sig)
I4<-mean(f(y)/dnorm(y,mu,sig))
eI4<-sd(f(y)/dnorm(y,mu,sig))/sqrt(N)
eS4<-(c(I4,eI4))
```

Veo el histograma del cociente de las funciones para ver como se distribuyeron los valores

```{r}
hist(f(y)/dnorm(y,mu,sig))
```

### Conclusiones para N = 10000

Veo los diferentes valores:
S(1), S(2), S(3), S(4)
```{r}
print(eS1)
print(eS2)
print(eS3)
print(eS4)
```
Estos tres estimadores son insesgados, lo que quiere decir que cuando n tienda a infinito los tres van a llegar al verdadero valor 14.66.

Pero lo que mas vale es el error de estimación, el menor. Que en este caso me dá el tercer caso, es donde optimizo los 10000 valores que simulé para hacer la estimación. 

En el caso 4 desperdicio muchos.(Ver gráfica inicial)

## Entonces la S(3) tiene la mejor aproximación porque el error de Monte Carlo es menor

  


